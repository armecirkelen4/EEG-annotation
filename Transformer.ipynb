{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer on spectrograms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3215, 3925)\n",
      "min max std of data: 0.0, 1.0, 0.0454247696949578\n",
      "patient ids: [ 28  28  28  28  28 100 100 100 100 100 100 100 100 100 100 100 100 100\n",
      " 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100]\n",
      "split ids: [28 28 28 28 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# load the combined features from the pickle file\n",
    "dir = \"E:/ChristianMusaeus/Data/Eyes_closed_marked\"\n",
    "\n",
    "# with open(f\"{dir}/hospital_spectrograms.pkl\", \"rb\") as f:\n",
    "#     data = pickle.load(f)\n",
    "    \n",
    "# load csv hospital data\n",
    "data = pd.read_csv(f\"{dir}/hospital_spectrograms.csv\")\n",
    "\n",
    "\n",
    "labels = data.iloc[:,-2].values\n",
    "patient_ids = data.iloc[:,-1].values\n",
    "data = data.iloc[:,:-4].values\n",
    "print(data.shape)\n",
    "print(f\"min max std of data: {np.min(data)}, {np.max(data)}, {np.std(data)}\")\n",
    "\n",
    "\n",
    "split_ids = patient_ids.copy() # Make split ids to keep track of the split of the data\n",
    "\n",
    "# make patient ids unique\n",
    "for i in range(1740, 3215):\n",
    "    patient_ids[i] += 100\n",
    "\n",
    "print(f\"patient ids: {patient_ids[1735:1770]}\")\n",
    "print(f\"split ids: {split_ids[1735:1770]}\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochTransformer(nn.Module):\n",
    "    def __init__(self, n_positions, n_embedding, n_datapoints, num_heads, num_layers):\n",
    "        super(EpochTransformer, self).__init__()\n",
    "        self.L = 10000\n",
    "        self.n_positions = n_positions\n",
    "        self.n_embedding = n_embedding\n",
    "        self.n_datapoints = n_datapoints\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=self.n_embedding, nhead=num_heads, dim_feedforward=1024)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "\n",
    "    def positional_encoding(self, n_positions, n_datapoints, L=10000):\n",
    "        \"\"\"\n",
    "        Generates positional encodings directly to match the size of the data matrix.\n",
    "\n",
    "        Args:\n",
    "            n_positions (int): Number of positions (sequence length).\n",
    "            n_datapoints (int): Number of datapoints (embedding size per epoch).\n",
    "            L (int): Maximum sequence length (default 10000).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Positional encoding matrix of shape (n_positions, n_datapoints).\n",
    "        \"\"\"\n",
    "        n = torch.arange(n_positions)[:, None]  # (n_positions, 1)\n",
    "        i = torch.arange(n_datapoints)[None, :]  # (1, n_datapoints)\n",
    "\n",
    "        # Compute positional encodings using the vectorized formula\n",
    "        angle_rates = 1 / (L ** (i / n_datapoints))\n",
    "        angle_rads = n * angle_rates\n",
    "\n",
    "        # Apply sin to even indices and cos to odd indices\n",
    "        encoding = torch.zeros_like(angle_rads)\n",
    "        encoding[:, 0::2] = torch.sin(angle_rads[:, 0::2])\n",
    "        encoding[:, 1::2] = torch.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        return encoding\n",
    "\n",
    "    def add_positional_encoding(self, data_matrix, L=10000):\n",
    "        \"\"\"\n",
    "        Adds positional encoding to the data matrix directly, matching the size of the matrix.\n",
    "\n",
    "        Args:\n",
    "            data_matrix (torch.Tensor): The data matrix of shape (batch_size, seq_len, n_datapoints).\n",
    "            L (int): Maximum sequence length (default 10000).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Data matrix with added positional encoding.\n",
    "        \"\"\"\n",
    "        # Extract the sequence length (seq_len) and embedding size (n_datapoints)\n",
    "        batch_size, n_positions, n_datapoints = data_matrix.shape  # Handle batch_size\n",
    "\n",
    "        # Generate positional encoding for the sequence length and embedding size\n",
    "        pos_enc = self.positional_encoding(n_positions, n_datapoints, L).to(data_matrix.device)\n",
    "        \n",
    "        # Add positional encoding to each sequence in the batch\n",
    "        pos_enc = pos_enc.unsqueeze(0).expand(batch_size, -1, -1)  # Expand to match batch size\n",
    "        \n",
    "        # Add the positional encoding to the data matrix\n",
    "        data_matrix_with_pos_enc = data_matrix + pos_enc\n",
    "        \n",
    "        return data_matrix_with_pos_enc\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.add_positional_encoding(x).float() # convert to float\n",
    "        output = self.transformer_encoder(x)\n",
    "        return output  # Compress into a feature vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceTransformer(nn.Module):\n",
    "    def __init__(self, seq_len, embedding_dim, num_heads, num_layers):\n",
    "        super(SequenceTransformer, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=1024)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "    \n",
    "    def add_sequence_positional_encoding(self, epoch_matrix, seq_len, embedding_dim):\n",
    "        \"\"\"\n",
    "        Adds positional encoding to the sequence of epochs.\n",
    "        Args:\n",
    "            epoch_matrix (torch.Tensor): The sequence of epoch feature vectors (batch_size, seq_len, embedding_dim)\n",
    "            seq_len (int): Length of the sequence (number of epochs).\n",
    "            embedding_dim (int): Embedding dimension per epoch.\n",
    "        Returns:\n",
    "            torch.Tensor: Sequence with added positional encoding.\n",
    "        \"\"\"\n",
    "        n = torch.arange(seq_len)[:, None]\n",
    "        i = torch.arange(embedding_dim)[None, :]\n",
    "        angle_rates = 1 / (10000 ** (i / embedding_dim))\n",
    "        pos_enc = torch.zeros((seq_len, embedding_dim))\n",
    "        pos_enc[:, 0::2] = torch.sin(n * angle_rates[:, 0::2]).to(epoch_matrix.device)\n",
    "        pos_enc[:, 1::2] = torch.cos(n * angle_rates[:, 1::2]).to(epoch_matrix.device)\n",
    "        \n",
    "        return epoch_matrix + pos_enc.to(epoch_matrix.device)\n",
    "\n",
    "\n",
    "    def forward(self, epoch_output_seq):\n",
    "        # Add positional encoding to the sequence of epoch outputs\n",
    "        encoded_seq = self.add_sequence_positional_encoding(epoch_output_seq, self.seq_len, self.embedding_dim).float()\n",
    "        # Pass through the transformer encoder\n",
    "        output_seq = self.transformer_encoder(encoded_seq)\n",
    "        return output_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalModel(nn.Module):\n",
    "    def __init__(self, n_positions, n_embedding, seq_len, num_heads, num_layers):\n",
    "        super(FinalModel, self).__init__()\n",
    "        # Initialize Epoch Transformer and Sequence Transformer\n",
    "        self.epoch_transformer = EpochTransformer(n_positions, n_embedding, n_embedding, num_heads, num_layers)\n",
    "        self.sequence_transformer = SequenceTransformer(seq_len, n_embedding, num_heads, num_layers)\n",
    "        # Fully connected layers for final classification\n",
    "        self.fc1 = nn.Linear(n_embedding, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)  # Binary classification\n",
    "\n",
    "    def forward(self, x, target_idx):\n",
    "        \"\"\"\n",
    "        Forward pass for the model.\n",
    "        Args:\n",
    "            x: Input of shape (batch_size, seq_len, n_datapoints)\n",
    "            target_idx: The index of the target epoch to classify.\n",
    "        \"\"\"\n",
    "        # Process the batch of epochs through the Epoch Transformer\n",
    "        batch_size, seq_len, n_datapoints = x.shape\n",
    "        \n",
    "        # Pass all epochs in the sequence through the Epoch Transformer\n",
    "        epoch_outputs = self.epoch_transformer(x)  # Shape: [batch_size, seq_len, n_embedding]\n",
    "        \n",
    "        # Process the entire sequence of epochs through the Sequence Transformer\n",
    "        sequence_output = self.sequence_transformer(epoch_outputs)  # Shape: [batch_size, seq_len, n_embedding]\n",
    "        \n",
    "        # Extract the specific epoch representation (e.g., epoch 6) from the sequence\n",
    "        target_epoch_output = sequence_output[:, target_idx]  # Shape: [batch_size, n_embedding]\n",
    "        \n",
    "        # Pass the target epoch's output through fully connected layers for classification\n",
    "        out = F.relu(self.fc1(target_epoch_output))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return F.sigmoid(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinalModel(\n",
      "  (epoch_transformer): EpochTransformer(\n",
      "    (transformer_encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=3925, out_features=3925, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=3925, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=3925, bias=True)\n",
      "          (norm1): LayerNorm((3925,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((3925,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (sequence_transformer): SequenceTransformer(\n",
      "    (transformer_encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=3925, out_features=3925, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=3925, out_features=1024, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=1024, out_features=3925, bias=True)\n",
      "          (norm1): LayerNorm((3925,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((3925,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=3925, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "139897155\n"
     ]
    }
   ],
   "source": [
    "model = FinalModel(n_positions=3215, n_embedding=3925, seq_len=11, num_heads=5, num_layers=1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "print(model)\n",
    "# import model summary\n",
    "from torchsummary import summary\n",
    "# model summary\n",
    "\n",
    "# summary(model, (11, 3925))\n",
    "# print number of parameters in m,odel\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(pytorch_total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1]) tensor([[0.3655],\n",
      "        [0.3676],\n",
      "        [0.3778],\n",
      "        [0.3557],\n",
      "        [0.3684]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create a batch of 10 sequences, each with 11 epochs and 6175 features\n",
    "batch_size = 5\n",
    "seq_len = 11  # We use 11 epochs in total (5 before, 1 target, 5 after)\n",
    "n_datapoints = 3925\n",
    "\n",
    "# Initialize the model\n",
    "model = FinalModel(n_positions=3215, n_embedding=n_datapoints, seq_len=seq_len, num_heads=5, num_layers=3)\n",
    "\n",
    "# Random input tensor of shape [batch_size, seq_len, n_datapoints]\n",
    "x = torch.randn(batch_size, seq_len, n_datapoints)\n",
    "\n",
    "# We want to classify epoch 6 \n",
    "target_idx = 5\n",
    "\n",
    "# Forward pass\n",
    "output = model(x, target_idx)\n",
    "print(output.shape, output)  # Should be [batch_size, 2] for binary classification\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "model = FinalModel(n_positions=3215, n_embedding=3925, seq_len=11, num_heads=5, num_layers=2).to(device)\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "epochs = data\n",
    "\n",
    "running_train_loss = []\n",
    "running_val_loss = []\n",
    "running_val_acc = []\n",
    "\n",
    "train_loops = 5\n",
    "\n",
    "\n",
    "for loop in range(train_loops):\n",
    "    print(f\"full data pass {loop+1} out of {train_loops}\")\n",
    "\n",
    "    for i , (train_index, test_index) in tqdm(enumerate(logo.split(epochs, labels, groups=split_ids))):\n",
    "        # Training iteriation\n",
    "        print(f\"Iteration {i+1} out of 29\")\n",
    "        with torch.enable_grad():\n",
    "            # Train and test split\n",
    "            X_train, X_val = epochs[train_index, :], epochs[test_index, :]\n",
    "            y_train, y_val = labels[train_index], labels[test_index]\n",
    "            # set data and labels to device and tensor\n",
    "            X_train, X_val = torch.tensor(X_train, dtype=torch.float32).to(device), torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "            y_train, y_val = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device), torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "            #print(f\"data label device: {X_train.device}, {y_train.device}\")\n",
    "            \n",
    "            # # Create DataLoader for training and testing\n",
    "            # train_dataset = EEGDataset(X_train, y_train)\n",
    "            # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            # val_dataset = EEGDataset(X_val, y_val)\n",
    "            # val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            loss_holder = []\n",
    "\n",
    "            for patient in np.unique(patient_ids[train_index]): # Right now we are testing on 3 patients, change to all patients when ready\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                patient_mask = torch.tensor(patient_ids[train_index] == patient, dtype=torch.bool)\n",
    "                patient_data = X_train[patient_mask].to(device)\n",
    "                patient_labels = y_train[torch.tensor(patient_ids[train_index] == patient, dtype=torch.bool)].to(device)\n",
    "                #print(f\"device of data and albels: {patient_data.device}, {patient_labels.device}\")\n",
    "                #print(f\"patient data: {patient_data.shape}\")\n",
    "                for i in range(5, len(patient_data)-5):\n",
    "                    output = model(patient_data[i-5:i+6].unsqueeze(0).to(device), 5).squeeze(0) # Moving window of 11 epochs, target is the 6th aka index 5\n",
    "                    #print(f\"output for patient {patient} epoch {i}: {output}\")\n",
    "                    loss = loss_fn(output, patient_labels[i].to(device))\n",
    "                    #print(f\"loss for patient {patient} epoch {i}: {loss}\")\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    loss_holder.append(loss.item())\n",
    "                    # make average of running loss across the entire training set\n",
    "            running_train_loss.append(np.mean(loss_holder))\n",
    "                    \n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_data = X_val\n",
    "            val_labels = y_val\n",
    "            for i in range(5, len(val_data)-5):\n",
    "                output = model(val_data[i-5:i+6].unsqueeze(0).to(device), 5).squeeze(0) # Moving window of 11 epochs, target is the 6th aka index 5\n",
    "                val_preds.append(output)\n",
    "                val_true.append(val_labels[i])\n",
    "\n",
    "        val_preds = torch.stack(val_preds).to(device)\n",
    "        val_true = torch.stack(val_true).to(device)\n",
    "        val_loss = loss_fn(val_preds, val_true)\n",
    "        running_val_loss.append(val_loss.item())\n",
    "        print(f\"Validation loss: {val_loss}\")\n",
    "\n",
    "        val_preds = (val_preds > 0.5).float()\n",
    "        val_true = val_true.float()\n",
    "\n",
    "        val_acc = accuracy_score(val_true.cpu().numpy(), val_preds.cpu().numpy())\n",
    "        running_val_acc.append(val_acc)\n",
    "        print(f\"Validation accuracy: {np.round(val_acc,2) * 100}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), \"E:/ChristianMusaeus/Data/Eyes_closed_marked/transformer_model.pth\")\n",
    "print(f\"model was saved to E:/ChristianMusaeus/Data/Eyes_closed_marked/transformer_model.pth\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CUDA not available\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "150000000*4/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
