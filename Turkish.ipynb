{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/gusta/OneDrive/Skrivebord/KI & Data/Bachelor/LegeData/S10_restingPre_EC.npy saved\n",
      "C:/Users/gusta/OneDrive/Skrivebord/KI & Data/Bachelor/LegeData/S10_restingPre_EO.npy saved\n",
      "C:/Users/gusta/OneDrive/Skrivebord/KI & Data/Bachelor/LegeData/S11_restingPre_EC.npy saved\n",
      "C:/Users/gusta/OneDrive/Skrivebord/KI & Data/Bachelor/LegeData/S11_restingPre_EO.npy saved\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dir = \"C:/Users/gusta/OneDrive/Skrivebord/KI & Data/Bachelor/LegeData\"\n",
    "# load matlab files \"S02_restingPre_EC.mat\", \"S02_restingPre_EO.mat\" and so on\n",
    "# and save them as numpy files\n",
    "from scipy.io import loadmat\n",
    "\n",
    "for i in [10,11]:\n",
    "    for j in [\"EC\", \"EO\"]:\n",
    "        data = loadmat(f\"{dir}/S{i}_restingPre_{j}.mat\")\n",
    "        np.save(f\"{dir}/S{i}_restingPre_{j}.npy\", data)\n",
    "        print(f\"{dir}/S{i}_restingPre_{j}.npy saved\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load numpy files\n",
    "import numpy as np\n",
    "dir = \"C:/Users/gusta/OneDrive/Skrivebord/KI & Data/Bachelor/LegeData\"\n",
    "\n",
    "# we have 10 patients, each with 2 conditions, load all of them\n",
    "data = {}\n",
    "for i in range(2,12):\n",
    "    for j in [\"EC\", \"EO\"]:\n",
    "        data[f\"S{i}_{j}\"] = np.load(f\"{dir}/S0{i}_restingPre_{j}.npy\", allow_pickle=True).item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/gusta/OneDrive/Skrivebord/KI & Data/Bachelor/LegeData/data.pkl saved\n"
     ]
    }
   ],
   "source": [
    "# save the data as a pickle file\n",
    "import pickle\n",
    "with open(f\"{dir}/data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)\n",
    "    print(f\"{dir}/data.pkl saved\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['S2_EC', 'S2_EO', 'S3_EC', 'S3_EO', 'S4_EC', 'S4_EO', 'S5_EC', 'S5_EO', 'S6_EC', 'S6_EO', 'S7_EC', 'S7_EO', 'S8_EC', 'S8_EO', 'S9_EC', 'S9_EO', 'S10_EC', 'S10_EO', 'S11_EC', 'S11_EO'])\n"
     ]
    }
   ],
   "source": [
    "# load the data from the pickle file\n",
    "\n",
    "import pickle\n",
    "dir = \"C:/Users/gusta/OneDrive/Skrivebord/KI & Data/Bachelor/LegeData\"\n",
    "with open(f\"{dir}/data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# print the keys of the data dictionary\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['dataRest'])\n"
     ]
    }
   ],
   "source": [
    "#remove all keys that are not \"dataRest\"\n",
    "for key in list(data.keys()):\n",
    "    for key2 in list(data[key].keys()):\n",
    "        if key2 != \"dataRest\":\n",
    "            del data[key][key2]\n",
    "print(data[\"S2_EC\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 68 channels, remove channel 65,66,67\n",
    "for key in data.keys():\n",
    "    data[key][\"dataRest\"] = np.delete(data[key][\"dataRest\"], [64,65,66], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "for key in data.keys():\n",
    "        # replace label values with 0 if it is smaller 210, else set it to 1\n",
    "        data[key][\"dataRest\"][-1] = np.where(data[key][\"dataRest\"][-1] < 210, 0, 1)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/gusta/OneDrive/Skrivebord/KI & Data/Bachelor/LegeData/data.pkl saved\n"
     ]
    }
   ],
   "source": [
    "# save data as a pickle file\n",
    "with open(f\"{dir}/data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)\n",
    "    print(f\"{dir}/data.pkl saved\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dir = \"C:/Users/gusta/OneDrive/Skrivebord/KI & Data/Bachelor/LegeData\"\n",
    "with open(f\"{dir}/data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150.00390625\n"
     ]
    }
   ],
   "source": [
    "data[\"S2_EC\"][\"dataRest\"].shape\n",
    "print(38401/256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2980, 65, 256)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have 150 seconds of data, with 256 samples per second, create 150 windows of 256 samples\n",
    "# each window is 1 second of data\n",
    "\n",
    "windows = []\n",
    "for key in data.keys():\n",
    "    for i in range(0, 38000, 256):\n",
    "        windows.append(data[key][\"dataRest\"][:,i:i+256])\n",
    "\n",
    "windows = np.array(windows)\n",
    "windows.shape\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(762880, 65)\n",
      "37120    1.0\n",
      "37121    1.0\n",
      "37122    1.0\n",
      "37123    1.0\n",
      "37124    1.0\n",
      "        ... \n",
      "39675    0.0\n",
      "39676    0.0\n",
      "39677    0.0\n",
      "39678    0.0\n",
      "39679    0.0\n",
      "Name: label, Length: 2560, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0    381440\n",
       "0.0    381440\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming `data_matrix` is your 3D data of shape (2980, 65, 256)\n",
    "data_matrix = windows\n",
    "# Step 1: Extract the data and labels\n",
    "# Separate the labels (last column in the 65th channel)\n",
    "labels = data_matrix[:, -1]  # Assuming labels are constant across all time points per window\n",
    "#print(labels[145:155])\n",
    "\n",
    "# Step 2: Reshape the data without the labels\n",
    "data_without_labels = data_matrix[:, :-1, :]  # Exclude the 65th channel (labels)\n",
    "\n",
    "# Reshape to (2980 * 256, 64)\n",
    "data_reshaped = data_without_labels.reshape(-1, data_without_labels.shape[1])\n",
    "\n",
    "# Step 3: Replicate the labels for each time point in the new rows\n",
    "# Repeat each label 256 times (once for each time point)\n",
    "\n",
    "\n",
    "# # Step 4: Combine the reshaped data and the repeated labels into a new DataFrame\n",
    "df = pd.DataFrame(data_reshaped, columns=[f'channel_{i}' for i in range(1, 65)])\n",
    "df['label'] = labels.reshape(-1)  # Add the repeated labels as a new column\n",
    "\n",
    "# # Check the shape of the final DataFrame\n",
    "print(df.shape)  # Should be (762880, 65)\n",
    "df.head()\n",
    "\n",
    "# Looks good\n",
    "print(df[\"label\"][145*256:155*256])\n",
    "df[\"label\"].value_counts()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/gusta/OneDrive/Skrivebord/KI & Data/Bachelor/LegeData/dataframe.pkl saved\n"
     ]
    }
   ],
   "source": [
    "# save the dataframe as a pickle file\n",
    "with open(f\"{dir}/dataframe.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df, f)\n",
    "    print(f\"{dir}/dataframe.pkl saved\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      channel_1     channel_2     channel_3     channel_4     channel_5  \\\n",
      "0  1.412883e+07  1.412985e+07  1.412550e+07  1.412387e+07  1.412431e+07   \n",
      "1  1.410380e+07  1.410847e+07  1.409964e+07  1.410329e+07  1.408847e+07   \n",
      "2  1.410851e+07  1.411286e+07  1.411599e+07  1.411535e+07  1.411033e+07   \n",
      "3  1.411343e+07  1.411062e+07  1.410969e+07  1.410703e+07  1.411014e+07   \n",
      "4 -4.909065e+06 -4.907337e+06 -4.912073e+06 -4.920873e+06 -4.917001e+06   \n",
      "\n",
      "      channel_6     channel_7     channel_8     channel_9    channel_10  ...  \\\n",
      "0  1.412505e+07  1.412047e+07  1.411859e+07  1.412438e+07  1.412313e+07  ...   \n",
      "1  1.409532e+07  1.410892e+07  1.411276e+07  1.411596e+07  1.410543e+07  ...   \n",
      "2  1.410796e+07  1.410323e+07  1.410652e+07  1.410252e+07  1.409711e+07  ...   \n",
      "3  1.410345e+07  1.411078e+07  1.410931e+07  1.411731e+07  1.411807e+07  ...   \n",
      "4 -4.908233e+06 -4.925417e+06 -4.918121e+06 -4.917065e+06 -4.916745e+06  ...   \n",
      "\n",
      "     channel_56    channel_57    channel_58    channel_59    channel_60  \\\n",
      "0  1.412755e+07  1.412419e+07  1.411747e+07  1.411785e+07  1.411919e+07   \n",
      "1  1.411238e+07  1.410867e+07  1.411078e+07  1.411590e+07  1.410063e+07   \n",
      "2  1.411030e+07  1.411823e+07  1.410979e+07  1.411644e+07  1.411267e+07   \n",
      "3  1.413107e+07  1.412857e+07  1.413196e+07  1.413676e+07  1.413833e+07   \n",
      "4 -4.922473e+06 -4.923849e+06 -4.930729e+06 -4.931945e+06 -4.929353e+06   \n",
      "\n",
      "     channel_61    channel_62    channel_63    channel_64  label  \n",
      "0  1.412815e+07  1.412067e+07  1.412233e+07  1.411027e+07    1.0  \n",
      "1  1.409907e+07  1.408953e+07  1.409385e+07  1.410323e+07    1.0  \n",
      "2  1.411110e+07  1.411388e+07  1.410287e+07  1.411766e+07    1.0  \n",
      "3  1.413759e+07  1.413065e+07  1.414159e+07  1.414284e+07    1.0  \n",
      "4 -4.923337e+06 -4.929001e+06 -4.925929e+06 -4.938601e+06    1.0  \n",
      "\n",
      "[5 rows x 65 columns]\n",
      "(762880, 65)\n"
     ]
    }
   ],
   "source": [
    "# load the dataframe from the pickle file\n",
    "import pickle\n",
    "dir = \"C:/Users/gusta/OneDrive/Skrivebord/KI & Data/Bachelor/LegeData\"\n",
    "with open(f\"{dir}/dataframe.pkl\", \"rb\") as f:\n",
    "    df = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
