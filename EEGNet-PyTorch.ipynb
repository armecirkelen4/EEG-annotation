{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "Original paper - https://arxiv.org/abs/1611.08024\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of patient 1: (np.float64(-2.5061664365916747), np.float64(3.2048110636610154))\n",
      "mean of patient 2: (np.float64(-2.2856199344616766), np.float64(3.1704214250154332))\n",
      "mean of patient 3: (np.float64(-2.6244058798465795), np.float64(2.326236025125405))\n",
      "mean of patient 4: (np.float64(-2.9839669159521462), np.float64(3.0033738566092985))\n",
      "mean of patient 5: (np.float64(-2.7180645928577865), np.float64(1.9617189652654394))\n",
      "mean of patient 6: (np.float64(-3.8235823428136357), np.float64(1.5538075585523163))\n",
      "mean of patient 7: (np.float64(-2.17836904765987), np.float64(2.700097334829556))\n",
      "mean of patient 8: (np.float64(-1.9391087092804788), np.float64(4.868952462591622))\n",
      "mean of patient 9: (np.float64(-3.278040789072896), np.float64(2.18437991679923))\n",
      "mean of patient 10: (np.float64(-1.6112388744411374), np.float64(4.696875071895533))\n",
      "size of df: (762880, 19)\n",
      "size of epochs: (2980, 256, 19)\n"
     ]
    }
   ],
   "source": [
    "# load the dataframe from the pickle file\n",
    "import pickle\n",
    "dir = \"C:/Users/gusta/OneDrive/Skrivebord/KI & Data/Bachelor/LegeData\"\n",
    "with open(f\"{dir}/dataframe.pkl\", \"rb\") as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "# keep these channels only, these are the indexes: \n",
    "\"\"\" \n",
    "Fp1 -> 0\n",
    "Fp2 -> 33\n",
    "F3  -> 4\n",
    "F4  -> 38\n",
    "C3  -> 12\n",
    "C4  -> 48\n",
    "P3  -> 20\n",
    "P4  -> 55\n",
    "O1  -> 26\n",
    "O2  -> 61\n",
    "F7  -> 6\n",
    "F8  -> 40\n",
    "T7  -> 14\n",
    "T8  -> 50\n",
    "P7  -> 22\n",
    "P8  -> 57\n",
    "Fz  -> 36\n",
    "Cz  -> 46\n",
    "Pz  -> 30 \n",
    "\n",
    "but add 1 to each index, since the first channel is channel_1\n",
    "\"\"\"\n",
    "\n",
    "df = df[[\"channel_1\", \"channel_34\", \"channel_5\", \"channel_39\", \"channel_13\", \"channel_49\", \"channel_21\", \"channel_56\", \"channel_27\", \"channel_62\", \"channel_7\", \"channel_41\", \"channel_15\", \"channel_51\", \"channel_23\", \"channel_58\", \"channel_37\", \"channel_47\", \"channel_31\", \"label\"]]\n",
    "\n",
    "patient_ids = np.repeat([1,2,3,4,5,6,7,8,9,10],76288)  # Make sure to have this aligned with your epochs/labels\n",
    "\n",
    "# Normalize per patient (within training and test sets)\n",
    "data = df.drop(\"label\", axis=1).values\n",
    "data_norm = []\n",
    "for patient_id in np.unique(patient_ids):\n",
    "    patient_data = data[patient_ids == patient_id]\n",
    "    scaler = StandardScaler()\n",
    "    patient_data_scaled = scaler.fit_transform(patient_data)\n",
    "    print(f\"mean of patient {patient_id}: {np.min(patient_data_scaled), np.max(patient_data_scaled)}\")\n",
    "    data_norm.append(patient_data_scaled)\n",
    "\n",
    "data_norm = np.concatenate(data_norm, axis=0)\n",
    "# add labels back\n",
    "y = df[\"label\"].values\n",
    "data = data_norm\n",
    "\n",
    "# make data a dataframe again\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"size of df: {df.shape}\")\n",
    "\n",
    "# split the data into epochs of 256 datapoints each\n",
    "epochs = []\n",
    "for i in range(0, len(df), 256):\n",
    "    epochs.append(df.iloc[i:i+256].values)\n",
    "\n",
    "# convert the list of epochs to a numpy array\n",
    "epochs = np.array(epochs)\n",
    "print(f\"size of epochs: {epochs.shape}\")\n",
    "\n",
    "# save the SCALED epochs\n",
    "with open(f\"{dir}/scaled_rawEEG_epochs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(epochs, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of labels: (2980,)\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for _ in range(10):\n",
    "    labels.append(np.repeat([1, 0], 149))\n",
    "labels = np.concatenate(labels)\n",
    "\n",
    "print(f\"size of labels: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4268]], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EEGNet(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(1, 19), stride=(1, 1))\n",
       "  (batchnorm1): BatchNorm2d(16, eps=False, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (padding1): ZeroPad2d((16, 17, 0, 1))\n",
       "  (conv2): Conv2d(1, 4, kernel_size=(2, 32), stride=(1, 1))\n",
       "  (batchnorm2): BatchNorm2d(4, eps=False, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pooling2): MaxPool2d(kernel_size=2, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  (padding2): ZeroPad2d((2, 1, 4, 3))\n",
       "  (conv3): Conv2d(4, 4, kernel_size=(8, 4), stride=(1, 1))\n",
       "  (batchnorm3): BatchNorm2d(4, eps=False, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pooling3): MaxPool2d(kernel_size=(2, 4), stride=(2, 4), padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels = 19\n",
    "sample_len = 256\n",
    "\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.T = sample_len\n",
    "        self.dropout_rate = 0.1\n",
    "        \n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, channels), padding = 0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "        self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
    "        \n",
    "        # Layer 3\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "        \n",
    "        # FC Layer\n",
    "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
    "        # I have 120 timepoints. \n",
    "        self.fc1 = nn.Linear(8*2 * (sample_len // 32), 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout(x, self.dropout_rate)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.dropout(x, self.dropout_rate)\n",
    "        x = self.pooling2(x)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.dropout(x, self.dropout_rate)\n",
    "        x = self.pooling3(x)\n",
    "        \n",
    "        # FC Layer\n",
    "        x = x.reshape(-1, 128)\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = EEGNet()\n",
    "print(net.forward(Variable(torch.Tensor(1,1,sample_len,channels))))\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "net.apply(init_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Define simple evaluation function\n",
    "def evaluate(model, dataloader, sample_len, channels):\n",
    "    model.eval()\n",
    "    model_predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for _, (data, target) in enumerate(dataloader):\n",
    "\n",
    "      inputs, targets = data , target\n",
    "      # permute inputs from (batch_size, channels, sample_len) to (batch_size, sample_len, channels)\n",
    "      inputs = inputs.permute(0, 2, 1)\n",
    "      inputs = inputs.reshape(-1,1,sample_len,channels)\n",
    "\n",
    "\n",
    "      data = Variable((inputs))\n",
    "      target = Variable((targets))\n",
    "      \n",
    "      output = model(data).reshape(-1)\n",
    "\n",
    "      model_predictions.extend(output.detach().numpy())\n",
    "      true_labels.extend(target.detach().numpy())\n",
    "\n",
    "    all_predictions = np.array(model_predictions) > 0.5\n",
    "    all_targets = np.array(true_labels)\n",
    "\n",
    "\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Dataset class definition\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, epochs, labels):\n",
    "        self.df = epochs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Access the epoch and corresponding label\n",
    "        data = self.df[idx]  # data shape: (256, 19)\n",
    "        label = self.labels[idx]  # label shape: ()\n",
    "        \n",
    "        # Ensure the data is converted to float tensor\n",
    "        data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "        \n",
    "        # Convert label to tensor (assumed to be scalar)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32) \n",
    "        \n",
    "        return data_tensor, label_tensor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.6458\n",
      "Train Accuracy: 50.22%\n",
      "Test Accuracy: 50.67%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.6458\n",
      "Train Accuracy: 49.44%\n",
      "Test Accuracy: 52.68%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.6458\n",
      "Train Accuracy: 49.89%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.6458\n",
      "Train Accuracy: 50.30%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.6458\n",
      "Train Accuracy: 49.78%\n",
      "Test Accuracy: 50.34%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.6458\n",
      "Train Accuracy: 49.81%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.6458\n",
      "Train Accuracy: 49.85%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.6458\n",
      "Train Accuracy: 49.96%\n",
      "Test Accuracy: 50.34%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.6458\n",
      "Train Accuracy: 50.00%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.6458\n",
      "Train Accuracy: 50.15%\n",
      "Test Accuracy: 50.00%\n",
      "Epoch 2\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.5637\n",
      "Train Accuracy: 50.00%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.5637\n",
      "Train Accuracy: 50.07%\n",
      "Test Accuracy: 50.34%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.5637\n",
      "Train Accuracy: 50.22%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.5637\n",
      "Train Accuracy: 50.00%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.5637\n",
      "Train Accuracy: 50.04%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.5637\n",
      "Train Accuracy: 50.00%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.5637\n",
      "Train Accuracy: 50.00%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.5637\n",
      "Train Accuracy: 49.96%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.5637\n",
      "Train Accuracy: 50.04%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.5637\n",
      "Train Accuracy: 50.04%\n",
      "Test Accuracy: 50.00%\n",
      "Epoch 3\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.4831\n",
      "Train Accuracy: 50.00%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.4831\n",
      "Train Accuracy: 50.04%\n",
      "Test Accuracy: 50.34%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.4831\n",
      "Train Accuracy: 50.04%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.4831\n",
      "Train Accuracy: 50.00%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.4831\n",
      "Train Accuracy: 50.04%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.4831\n",
      "Train Accuracy: 50.04%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.4831\n",
      "Train Accuracy: 50.19%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.4831\n",
      "Train Accuracy: 50.04%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.4831\n",
      "Train Accuracy: 50.04%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.4831\n",
      "Train Accuracy: 50.04%\n",
      "Test Accuracy: 50.00%\n",
      "Epoch 4\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.3275\n",
      "Train Accuracy: 50.00%\n",
      "Test Accuracy: 49.33%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.3275\n",
      "Train Accuracy: 49.85%\n",
      "Test Accuracy: 50.34%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.3275\n",
      "Train Accuracy: 50.15%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.3275\n",
      "Train Accuracy: 49.89%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.3275\n",
      "Train Accuracy: 50.07%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.3275\n",
      "Train Accuracy: 49.96%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.3275\n",
      "Train Accuracy: 50.00%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.3275\n",
      "Train Accuracy: 49.89%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.3275\n",
      "Train Accuracy: 50.11%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.3275\n",
      "Train Accuracy: 49.96%\n",
      "Test Accuracy: 50.00%\n",
      "Epoch 5\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.5013\n",
      "Train Accuracy: 50.04%\n",
      "Test Accuracy: 48.32%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 0.5013\n",
      "Train Accuracy: 49.85%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     56\u001b[0m running_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\gusta\\miniconda3\\envs\\BACHELOR\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gusta\\miniconda3\\envs\\BACHELOR\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gusta\\miniconda3\\envs\\BACHELOR\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "# Initialize Leave-One-Group-Out cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# This is an array where each entry corresponds to a patient ID (e.g., [0, 0, 0, 1, 1, 1, ..., 9])\n",
    "patient_ids = np.repeat([1,2,3,4,5,6,7,8,9,10],298)  # Make sure to have this aligned with your epochs/labels\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):  # Loop over the number of epochs\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    running_loss = []\n",
    "\n",
    "    for train_index, test_index in logo.split(epochs, labels, groups=patient_ids):\n",
    "        # Training loop\n",
    "        net.train()\n",
    "\n",
    "        # Train and test split\n",
    "        X_train, X_test = epochs[train_index,:,:], epochs[test_index,:,:]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        \n",
    "        # Create DataLoader for training and testing\n",
    "        train_dataset = EEGDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        test_dataset = EEGDataset(X_test, y_test)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            if i % 16 == 0:\n",
    "                print(i, end=\", \")\n",
    "            \n",
    "            # Get the inputs; data is a list of [inputs, labels/targets]\n",
    "            inputs, targets = data\n",
    "            inputs = inputs.reshape(-1, 1, sample_len, channels)  # Reshape for network input\n",
    "\n",
    "            # Convert inputs and targets to variables\n",
    "            inputs, targets = Variable(inputs), Variable(targets)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = net(inputs).reshape(-1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "            # Print training loss\n",
    "            print(f\"Training Loss: {running_loss[i]:.4f}\")\n",
    "        \n",
    "        # Check performance on training and testing sets\n",
    "        train_accuracy = evaluate(net, train_loader, sample_len, channels)\n",
    "        test_accuracy = evaluate(net, test_loader, sample_len, channels)\n",
    "        \n",
    "        print(f\"Train Accuracy: {train_accuracy*100:.2f}%\")\n",
    "        print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BACHELOR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
