{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Written by, \n",
    "Sriram Ravindran, sriram@ucsd.edu\n",
    "\n",
    "Original paper - https://arxiv.org/abs/1611.08024\n",
    "\n",
    "Please reach out to me if you spot an error.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of df: (762880, 19)\n",
      "size of epochs: (2980, 256, 19)\n"
     ]
    }
   ],
   "source": [
    "# load the dataframe from the pickle file\n",
    "import pickle\n",
    "dir = \"C:/Users/gusta/OneDrive/Skrivebord/KI & Data/Bachelor/LegeData\"\n",
    "with open(f\"{dir}/dataframe.pkl\", \"rb\") as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "# keep these channels only, these are the indexes: \n",
    "\"\"\" \n",
    "Fp1 -> 0\n",
    "Fp2 -> 33\n",
    "F3  -> 4\n",
    "F4  -> 38\n",
    "C3  -> 12\n",
    "C4  -> 48\n",
    "P3  -> 20\n",
    "P4  -> 55\n",
    "O1  -> 26\n",
    "O2  -> 61\n",
    "F7  -> 6\n",
    "F8  -> 40\n",
    "T7  -> 14\n",
    "T8  -> 50\n",
    "P7  -> 22\n",
    "P8  -> 57\n",
    "Fz  -> 36\n",
    "Cz  -> 46\n",
    "Pz  -> 30 \n",
    "\n",
    "but add 1 to each index, since the first channel is channel_1\n",
    "\"\"\"\n",
    "\n",
    "df = df[[\"channel_1\", \"channel_34\", \"channel_5\", \"channel_39\", \"channel_13\", \"channel_49\", \"channel_21\", \"channel_56\", \"channel_27\", \"channel_62\", \"channel_7\", \"channel_41\", \"channel_15\", \"channel_51\", \"channel_23\", \"channel_58\", \"channel_37\", \"channel_47\", \"channel_31\", \"label\"]]\n",
    "\n",
    "patient_ids = np.repeat([1,2,3,4,5,6,7,8,9,10],76288)  # Make sure to have this aligned with your epochs/labels\n",
    "\n",
    "# Normalize per patient (within training and test sets)\n",
    "data = df.drop(\"label\", axis=1).values\n",
    "data_norm = []\n",
    "for patient_id in np.unique(patient_ids):\n",
    "    patient_data = data[patient_ids == patient_id]\n",
    "    scaler = StandardScaler()\n",
    "    patient_data_scaled = scaler.fit_transform(patient_data)\n",
    "    data_norm.append(patient_data_scaled)\n",
    "\n",
    "data_norm = np.concatenate(data_norm, axis=0)\n",
    "# add labels back\n",
    "y = df[\"label\"].values\n",
    "data = data_norm\n",
    "\n",
    "# make data a dataframe again\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"size of df: {df.shape}\")\n",
    "\n",
    "# split the data into epochs of 256 datapoints each\n",
    "epochs = []\n",
    "for i in range(0, len(df), 256):\n",
    "    epochs.append(df.iloc[i:i+256].values)\n",
    "\n",
    "# convert the list of epochs to a numpy array\n",
    "epochs = np.array(epochs)\n",
    "print(f\"size of epochs: {epochs.shape}\")\n",
    "\n",
    "# save the SCALED epochs\n",
    "with open(f\"{dir}/scaled_rawEEG_epochs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(epochs, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of labels: (2980,)\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for _ in range(10):\n",
    "    labels.append(np.repeat([1, 0], 149))\n",
    "labels = np.concatenate(labels)\n",
    "\n",
    "print(f\"size of labels: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6791]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "channels = 19\n",
    "sample_len = 256\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.T = 120\n",
    "        \n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, channels), padding = 0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "        self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
    "        \n",
    "        # Layer 3\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "        \n",
    "        # FC Layer\n",
    "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
    "        # I have 120 timepoints. \n",
    "        self.fc1 = nn.Linear(8*2 * (sample_len // 32), 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = self.pooling2(x)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = self.batchnorm3(x)\n",
    "        x = F.dropout(x, 0.25)\n",
    "        x = self.pooling3(x)\n",
    "        \n",
    "        # FC Layer\n",
    "        x = x.reshape(-1, 128)\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = EEGNet()\n",
    "print(net.forward(Variable(torch.Tensor(1,1,sample_len,channels))))\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate function returns values of different criteria like accuracy, precision etc. \n",
    "In case you face memory overflow issues, use batch size to control how many samples get evaluated at one time. Use a batch_size that is a factor of length of samples. This ensures that you won't miss any samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate(model, data_loader, sample_len, channels):\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
    "        for data in data_loader:\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, test_targets = data\n",
    "            # permute inputs from (batch_size, channels, sample_len) to (batch_size, sample_len, channels)\n",
    "            inputs = inputs.permute(0, 2, 1)\n",
    "            inputs = inputs.reshape(-1,1,sample_len,channels)\n",
    "\n",
    "            # wrap them in Variable\n",
    "            inputs, test_targets = Variable(inputs), Variable(test_targets)\n",
    "            \n",
    "            # Get model predictions\n",
    "            outputs = model(inputs)\n",
    "            predictions = outputs.data.cpu().numpy()\n",
    "            targets = test_targets.data.cpu().numpy()\n",
    "            \n",
    "            all_predictions.extend(np.round(predictions))\n",
    "            all_targets.extend(targets)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    acc = accuracy_score(all_targets, all_predictions)\n",
    "    return f\"accuracy = {acc * 100:.2f}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate random data\n",
    "\n",
    "##### Data format:\n",
    "Datatype - float32 (both X and Y) <br>\n",
    "X.shape - (#samples, 1, #timepoints,  #channels) <br>\n",
    "Y.shape - (#samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "channels = 19\n",
    "sample_len = 256\n",
    "no_samples = 2980\n",
    "\n",
    "X_train = np.random.rand(no_samples, 1, sample_len, channels).astype('float32') # np.random.rand generates between [0, 1)\n",
    "y_train = np.round(np.random.rand(no_samples).astype('float32')) # binary data, so we round it to 0 or 1.\n",
    "\n",
    "X_val = np.random.rand(no_samples, 1, sample_len, channels).astype('float32')\n",
    "y_val = np.round(np.random.rand(no_samples).astype('float32'))\n",
    "\n",
    "X_test = np.random.rand(no_samples, 1, sample_len, channels).astype('float32')\n",
    "y_test = np.round(np.random.rand(no_samples).astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Dataset class definition\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, epochs, labels):\n",
    "        self.df = epochs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Access the epoch and corresponding label\n",
    "        data = self.df[idx]  # data shape: (256, 19)\n",
    "        label = self.labels[idx]  # label shape: ()\n",
    "        \n",
    "        # Ensure the data is converted to float tensor\n",
    "        data_tensor = torch.tensor(data, dtype=torch.float32)\n",
    "        \n",
    "        # Convert label to tensor (assumed to be scalar)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32) \n",
    "        \n",
    "        return data_tensor, label_tensor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 78.1130\n",
      "Train Accuracy: 50.07%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 77.1403\n",
      "Train Accuracy: 50.11%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 77.2018\n",
      "Train Accuracy: 50.11%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 77.7883\n",
      "Train Accuracy: 49.96%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 73.4538\n",
      "Train Accuracy: 49.81%\n",
      "Test Accuracy: 57.72%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 76.3760\n",
      "Train Accuracy: 50.07%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 69.4236\n",
      "Train Accuracy: 49.81%\n",
      "Test Accuracy: 50.00%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 74.1980\n",
      "Train Accuracy: 50.37%\n",
      "Test Accuracy: 45.30%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 71.1298\n",
      "Train Accuracy: 51.12%\n",
      "Test Accuracy: 52.01%\n",
      "0, 16, 32, 48, 64, 80, 96, 112, 128, 144, 160, Training Loss: 72.5159\n",
      "Train Accuracy: 50.52%\n",
      "Test Accuracy: 51.34%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "# Initialize Leave-One-Group-Out cross-validator\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "# This is an array where each entry corresponds to a patient ID (e.g., [0, 0, 0, 1, 1, 1, ..., 9])\n",
    "patient_ids = np.repeat([1,2,3,4,5,6,7,8,9,10],298)  # Make sure to have this aligned with your epochs/labels\n",
    "\n",
    "for train_index, test_index in logo.split(epochs, labels, groups=patient_ids):\n",
    "    \n",
    "    # Train and test split\n",
    "    X_train, X_test = epochs[train_index,:,:], epochs[test_index,:,:]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    # Create DataLoader for training and testing\n",
    "    train_dataset = EEGDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = EEGDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Training loop\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        if i % 16 == 0:\n",
    "            print(i, end=\", \")\n",
    "        \n",
    "        # Get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.reshape(-1, 1, sample_len, channels)  # Reshape for network input\n",
    "\n",
    "        # Convert inputs and targets to variables\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs).reshape(-1)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print training loss\n",
    "    print(f\"Training Loss: {running_loss:.4f}\")\n",
    "    \n",
    "    # Check performance on training and testing sets\n",
    "    train_accuracy = evaluate(net, train_loader, sample_len, channels)\n",
    "    test_accuracy = evaluate(net, test_loader, sample_len, channels)\n",
    "    \n",
    "    print(f\"Train Accuracy: {float(train_accuracy.split('=')[1].strip('%')):.2f}%\")\n",
    "    print(f\"Test Accuracy: {float(test_accuracy.split('=')[1].strip('%')):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BACHELOR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
